任务场景：
四旋翼无人机颠球（顶部固定球拍）。球从uav正上方附近落下，uav控球（平稳地竖直颠球）。可能的失误导致球向侧面飞出，uav去救球。救到后先稳住、消除球的水平速度。然后慢慢带回一个接近原点的位置，继续控球，循环往复。

action：ctbr（油门和角速度指令）
obs：无人机

good case
需要救球的时候快速去救球，然后把球控住（竖着颠球）。如果位置偏离得多，再"带球"慢慢颠回中心。

bad case
1. 不会救球（球有较大水平速度时）
1.1 当球向侧面飞出时不去救
1.2 球向侧面飞出时(如vx=0.5m/s)，无人机跟着球走，而不会尝试消除球已有的水平速度(让vx归零)
2. 不会控球（球几乎没有水平速度时）
2.1 姿态摇晃着击球，非常容易失误
2.2 一会用拍子左侧击球，一会用右侧，位置（左右）晃来晃去，容易失误

需要防止遗忘，保证一直有救球和控球的考验。通过随机无人机初始位置、速度，随机球的初始位置、速度。尤其较大的随机初始球速？
································································································
    基本击球 (Basic Hit): 在球正上方落下时，能用球拍中心稳定地将球向上击打。

    稳定控球 (Stable Control): 连续、垂直地颠球，保持无人机姿态平稳，并将球维持在场地中心附近。

    快速救球 (Reactive Save): 当球因失误产生较大水平速度飞出时，能快速移动到球的落点下方，并准备拦截。

    消除侧向速度 (Cancel Horizontal Velocity): 在救球时，通过精确的角度和时机击球，消除球的水平速度，使其转为垂直运动。

    带球回中 (Dribbling Back to Center): 在成功稳住球后，如果位置偏离中心，能通过微调击球角度，一拍一拍地将球“带”回中心区域。
·······························································································
核心目标：击球奖励 R_hit

R_hit 在每次无人机与球发生接触时触发。一个设计良好的 R_hit 不仅仅是“打到了就行”，而是要奖励“高质量”的击球。

R_hit 的引导性设计：

R_hit = w_v * R_vertical_velocity + w_c * R_center_hit + w_s * R_stability_hit

    R_vertical_velocity (奖励垂直速度):

        目标: 鼓励将球笔直向上打，并给予足够的高度，为下次击球留出反应时间。

        实现: R_vertical_velocity = clip(ball_vz_after_hit, 0, V_max)

            ball_vz_after_hit 是击球后球的瞬时垂直速度。

            使用 clip 函数忽略向下的速度（这是失败的击球），并设置一个上限 V_max 防止奖励无限增大。

        作用: 直接引导智能体学习控球，让球飞得高、飞得正。

    R_center_hit (奖励中心击球):

        目标: 解决 bad case 2.2（用拍子边缘击球）。中心击球更稳定，方向更可控。

        实现: R_center_hit = exp(-k_dist * distance_to_racket_center)

            distance_to_racket_center 是击球点到球拍中心的距离。

            使用指数函数，距离越近，奖励越高（趋近于1）；距离越远，奖励迅速衰减。

        作用: 提升控球的稳定性和精确性。

    R_stability_hit (奖励稳定姿态击球):

        目标: 解决 bad case 2.1（姿态摇晃着击球）。

        实现: R_stability_hit = exp(-k_angle * (drone_roll^2 + drone_pitch^2))

            drone_roll, drone_pitch 是击球瞬间无人机的滚转和俯仰角。

            无人机越水平，该奖励越高。

        作用: 确保击球动作干净利落，是控球和救球成功的基础。

持续性奖励/塑形奖励 (Shaping Rewards)

这些奖励在每个时间步都会计算，用于引导智能体在非击球时刻的行为。

    R_approach (接近球奖励):

        目标: 引导无人机移动到球的下方。

        实现: R_approach = -distance(racket_pos, ball_pos_projected)

            racket_pos 是球拍的(x,y)位置。

            ball_pos_projected 是球在球拍平面上的投影位置。

            此奖励只在球高于球拍时给予，防止无人机从上方撞球。

        作用: 这是救球和控球的第一步。

    R_save (救球奖励/消除水平速度奖励):

        目标: 解决 bad case 1.2（只会跟着球走，不会消除水平速度）。

        实现: 在击球后计算 R_save = - (ball_vx_after_hit^2 + ball_vy_after_hit^2)。

            这是一个惩罚项。击球后球的水平速度越小，惩罚越小（奖励相对越高）。

        作用: 这是引导“救球”行为最关键的奖励信号。 它直接告诉智能体：“你的目标不只是碰到球，而是要让球停下来（水平方向）。”

    R_centering (带球回中奖励):

        目标: 引导智能体在稳住球后，慢慢带回中心。

        实现: R_centering = - (ball_x^2 + ball_y^2)

            惩罚球偏离场地中心(0,0)的距离。

        作用: 引导智能体完成“带球回中”的高级任务。

    R_survival (存活奖励):

        目标: 鼓励持续颠球。

        实现: 每个时间步给予一个小的正常数，+0.1。

        作用: 激励智能体尽可能延长颠球时间。

    R_control_effort (控制努力惩罚):

        目标: 鼓励平滑的动作，避免无人机剧烈晃动。

        实现: R_control_effort = -norm(action)^2

        作用: 提升整体飞行的稳定性。

总奖励函数

R_total = R_survival + w_approach*R_approach + w_centering*R_centering + w_effort*R_control_effort

在击球瞬间，额外加上：
R_total_on_hit = R_total + R_hit + w_save*R_save

权重 w_* 需要仔细调整，这通常是RL工程中最耗时的一步。
·························································································
    关键在于权重 w_save 和 w_stability_hit 的设定。 必须保证 w_save 足够大，w_stability_hit 相对较小。

    R_stability_hit = exp(-k_angle * (roll^2 + pitch^2)) 这个指数形式本身就很有利：

        对于小角度的、必要的倾斜，roll^2 + pitch^2很小，exp(...)接近1，惩罚非常轻微。

        对于大角度的、不稳定的摇晃，roll^2 + pitch^2很大，exp(...)迅速趋近于0，惩罚非常严重。

    这完美地实现了我们的目标： 它允许为了救球而产生的受控的小倾斜，同时严厉惩罚了不稳定的剧烈摇晃。

···························································································
随机化球的初始速度 让无人机脱离舒适区？
加随机化应该保证简单case（头顶下落）？加了之后，发现头顶下落的更不好了，怎么办
···························································································
如何归一化？

    对于有界的奖励 (如速度): normalized_r = R_vertical_velocity / V_max。将其缩放到 [0, 1]。

    对于距离/误差类惩罚 (天然为负): 使用指数函数，效果极佳。normalized_r = exp(-k * distance)。这会将 [0, ∞) 的距离映射到 (0, 1] 的奖励。k是调节灵敏度的系数。

    对于姿态稳定奖励: exp(-k * (roll^2 + pitch^2)) 本身就是一种归一化，结果在 (0, 1] 之间。

    对于常数奖励 (如survival): 让它保持为一个小的常数即可，它起到一个“时间基准”的作用。
·······································
    w_approach: 这是第一个需要调试的权重。一个好的出发点是：在一个典型的“去救球”过程中，累积的 R_approach 奖励应该与一次成功击球的 R_hit 奖励量级相当。

    例如：如果一次高质量击球 R_hit ≈ 5，而智能体需要20个时间步去接近球，那么每一步的 w_approach * R_approach 应该在 5/20 = 0.25 左右。如果R_approach归一化到1，那么w_approach可以从 0.25 左右开始尝试。
·······································
w_save: 这个权重必须足够大，大到它带来的奖励（或避免的惩罚）足以抵消R_stability_hit带来的倾斜惩罚。可以从与w_hit相当的值开始，比如 1.0，甚至更高 2.0。

w_stability_hit: 这个是“软约束”，权重不宜过高。它的作用是让智能体在“能完成任务”和“姿态稳定”之间找到一个平衡。可以从一个较小的值开始，比如 0.1 - 0.2。

w_centering: 这是最“奢侈”的目标，只有在球被完全控制住后才应该考虑。因此它的权重应该是最低的。比如 0.05。它像一个微弱但持续的引力，慢慢把球拉回中心
·············································
一份可以作为起点的权重清单
奖励项	目的	归一化建议	起始权重 w	调试观察点
R_hit	核心目标	组合奖励，内部加权	1.0 (基准)	智能体是否有强烈的击球欲望？
R_survival	鼓励持续	小的正常数	0.01	是否会过早放弃？如果会，稍微调高。
R_approach	引导移动	exp(-k * dist)	0.2 - 0.5	是否能主动移动到球下方？太高会导致不敢离球。
R_save	救球核心	- (vx^2 + vy^2)	1.0 - 2.0	面对侧飞的球，是跟着走还是尝试拦截？权重必须足够高才能学会拦截。
R_stability_hit	姿态约束	exp(-k * angle^2)	0.1 - 0.3	是否过度摇晃？太高会妨碍救球，太低则姿态不稳定。
R_centering	长期优化	- (x^2 + y^2)	0.05	在稳定控球后，是否有向中心移动的趋势？
R_control_effort	动作平滑	- norm(action)^2	0.001	动作是否过于剧烈？这是一个非常微调的项。
·········································



a. R_hit (击球奖励) - 保持不变

依然是核心，奖励向上的击球，惩罚水平方向的击球。
R_hit = C_vz * v_ball_z - C_vxy * ||v_ball_xy||
b. R_control (控球/持球奖励) - 稳定阶段

这个奖励在球处于“可控”状态时生效，鼓励无人机把球稳定在中心区域。

    定义可控状态： 球的水平速度很小（例如 ||v_ball_xy|| < v_thresh），且无人机接近球的下方。

    奖励内容：

        球的位置奖励 (R_ball_pos): 当球在中心区域 (0, 0) 附近时，给予奖励。可以用高斯函数：
        R_ball_pos = exp(- (||ball_pos.xy - center_pos.xy||^2) / sigma_ball^2)

        无人机位置奖励 (R_drone_pos): 也鼓励无人机自身保持在中心区域。
        R_drone_pos = exp(- (||drone_pos.xy - center_pos.xy||^2) / sigma_drone^2)

        组合: R_control = C_ball_pos * R_ball_pos + C_drone_pos * R_drone_pos